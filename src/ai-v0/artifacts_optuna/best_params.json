{
  "n_layers": 3,
  "n_units_l0": 82,
  "n_units_l1": 83,
  "n_units_l2": 246,
  "alpha": 0.030563949269376542,
  "learning_rate_init": 0.0005792219377088149,
  "batch_size": 128,
  "activation": "relu"
}